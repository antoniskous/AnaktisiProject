{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059508b5-0d24-4687-98a7-417f26865ac9",
   "metadata": {},
   "source": [
    "#ΑΝΑΚΤΗΣΗ ΠΛΗΡΟΦΟΡΙΑΣ PROJECT\n",
    "##ΟΝΟΜΑΤΕΠΩΝΥΜΑ:1.ΚΟΥΣΑΘΑΝΑΣ ΑΝΤΩΝΙΟΣ, 2.ΖΓΟΥΛΕΤΑΣ ΣΤΕΡΓΙΑΝΟΣ\n",
    "##ΑΜ1:713242017058 , ΑΜ2:19390060\n",
    "##LINK GITHUB : https://github.com/antoniskous/AnaktisiProject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee23678-9727-4d97-9563-cb74f3e5138b",
   "metadata": {},
   "source": [
    "#CRAWLER2.PY AΡΧΕΙΟ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd128c90-9bee-4737-97b9-c14dc016355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ΦΟΡΤΩΣΗ ΒΙΒΛΙΟΘΗΚΩΝ ΚΑΙ ΠΡΟΕΠΕΞΕΡΓΑΣΙΑ ΚΕΙΜΕΝΟΥ\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "\n",
    "# Εγκατάσταση nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "##ΠΡΟΕΠΕΞΕΡΓΑΣΙΑ ΚΕΙΜΕΝΟΥ\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Επεξεργάζεται το κείμενο:\n",
    "    - Αφαιρεί μη-αλφαριθμητικούς χαρακτήρες.\n",
    "    - Χωρίζει το κείμενο σε tokens.\n",
    "    - Αφαιρεί stopwords.\n",
    "    - Εφαρμόζει lemmatization.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Stop-word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Παράδειγμα\n",
    "example_text = \"Artificial Intelligence is the future of technology!\"\n",
    "print(\"Preprocessed tokens:\", preprocess_text(example_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a94ff-b904-44bf-9014-81f7b4e42ca5",
   "metadata": {},
   "source": [
    "#ΑΝΑΖΗΤΗΣΗ ΚΑΙ ΕΞΑΓΩΓΗ ΑΡΘΡΩΝ. ΠΙΟ ΣΥΓΚΕΚΡΙΜΕΝΑ ΤΟ 'crawl_wikipedia' είναι για την αναζήτηση και το 'extract_wikipedia_article' για την εξαγωγή των τίτλων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366d1ff-c70d-4ca7-a694-30a45c73e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_wikipedia(query, max_results=5):\n",
    "   \n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": query,\n",
    "        \"format\": \"json\",\n",
    "        \"srlimit\": max_results\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        search_results = response.json().get('query', {}).get('search', [])\n",
    "        articles = []\n",
    "        \n",
    "        for result in search_results:\n",
    "            title = result['title']\n",
    "            page_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "            articles.append(extract_wikipedia_article(page_url))\n",
    "        \n",
    "        return articles\n",
    "    else:\n",
    "        print(f\"Failed to fetch search results for query: {query}\")\n",
    "        return []\n",
    "\n",
    "def extract_wikipedia_article(url):\n",
    "   \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1', {'id': 'firstHeading'}).text.strip()\n",
    "        content = ''\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            content += paragraph.text.strip() + ' '\n",
    "\n",
    "        preprocessed_title = preprocess_text(title)\n",
    "        preprocessed_content = preprocess_text(content)\n",
    "\n",
    "        print(f\"Title: {title}, Preprocessed Title: {preprocessed_title}\")\n",
    "        print(f\"Content Sample: {content[:100]}, Preprocessed Content: {preprocessed_content[:10]}\")\n",
    "\n",
    "        article = {\n",
    "            'url': url,\n",
    "            'title': title,\n",
    "            'content': content,\n",
    "            'preprocessed_title': preprocessed_title,\n",
    "            'preprocessed_content': preprocessed_content\n",
    "        }\n",
    "\n",
    "        return article\n",
    "    else:\n",
    "        print(f\"Failed to fetch article from {url}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f793c-ca14-4df2-bc5f-33c5a98bdaca",
   "metadata": {},
   "source": [
    "#ΔΗΜΙΟΥΡΓΙΑ ΚΑΙ ΑΠΟΘΗΚΕΥΣΗ ΑΝΤΡΙΣΤΡΟΦΟΥ ΕΥΡΕΤΗΡΙΟΥ 'create_and_save_reverse_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88beb792-d08f-4c6a-9408-473265359e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_reverse_index(articles):\n",
    "  \n",
    "    reverse_index = defaultdict(list)\n",
    "\n",
    "    for idx, article in enumerate(articles):\n",
    "        for term in set(article['preprocessed_title'] + article['preprocessed_content']):\n",
    "            reverse_index[term].append(idx)\n",
    "\n",
    "    print(f\"Reverse Index Sample: {dict(list(reverse_index.items())[:10])}\")\n",
    "    save_reverse_index(reverse_index)\n",
    "\n",
    "def save_to_json(articles, filename='wikipedia_articles.json'):\n",
    "   \n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(articles, json_file, indent=2)\n",
    "    print(f'Saved {len(articles)} articles to {filename}')\n",
    "\n",
    "def save_reverse_index(reverse_index, filename='reverse_index.json'):\n",
    "  \n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(reverse_index, json_file, indent=2)\n",
    "    print(f'Saved reverse index to {filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2b938-20ca-4bf0-ab33-423ce312c054",
   "metadata": {},
   "source": [
    "#ΕΚΤΕΛΕΣΗ ΠΑΡΑΔΕΙΓΜΑΤΟΣ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1645fa-ff2f-4f11-ab69-b75b1da2dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Εκτέλεση του crawler\n",
    "query = \"Artificial Intelligence\"\n",
    "wikipedia_articles = crawl_wikipedia(query)\n",
    "\n",
    "# Αποθήκευση άρθρων και ευρετηρίου\n",
    "save_to_json(wikipedia_articles)\n",
    "create_and_save_reverse_index(wikipedia_articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3fafd5-a66b-47b1-8784-6041d63dbdf2",
   "metadata": {},
   "source": [
    "#ΠΡΟΕΠΕΞΕΡΓΑΣΙΑ ΚΕΙΜΕΝΟΥ\n",
    "ΒΑΣΙΚΕΣ ΛΕΙΤΟΥΡΓΙΕΣ:\n",
    "1.ΑΦΑΙΡΕΣΗ ΜΗ-ΑΛΦΑΡΙΘΜΗΤΙΚΩΝ ΧΑΡ/ΡΩΝ\n",
    "2.TOKENIZATION\n",
    "3.AΦΑΙΡΕΣΗ STOPWORDS.\n",
    "4.LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba649e-931f-41b5-9a2f-5e59cbabf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEARCH.PY AΡΧΕΙΟ\n",
    "#ΕΙΣΑΓΩΓΗ ΒΙΒΛΙΟΘΗΚΩΝ ΚΑΙ ΦΟΡΤΩΣΗ ΔΕΔΟΜΕΝΩΝ\n",
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score\n",
    "import nltk\n",
    "\n",
    "# Εγκατάσταση nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Φόρτωση δεδομένων\n",
    "def load_articles(filename='wikipedia_articles.json'):\n",
    "    with open(filename, 'r') as json_file:\n",
    "        articles = json.load(json_file)\n",
    "    return articles\n",
    "\n",
    "def load_reverse_index(filename='reverse_index.json'):\n",
    "    with open(filename, 'r') as json_file:\n",
    "        reverse_index = json.load(json_file)\n",
    "    return reverse_index\n",
    "\n",
    "articles = load_articles()\n",
    "reverse_index = load_reverse_index()\n",
    "\n",
    "print(f\"Loaded {len(articles)} articles.\")\n",
    "print(f\"Sample reverse index keys: {list(reverse_index.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9a81dd-ec9f-4053-8d63-807145036c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "  \n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664761a7-03c5-4853-8f1e-fa9542b9a2b7",
   "metadata": {},
   "source": [
    "##ΥΛΟΠΟΙΗΣΗ ΑΛΓΟΡΙΘΜΩΝ ΑΝΑΖΗΤΗΣΗΣ\n",
    "1.BOOLEAN\n",
    "2.VSM\n",
    "3.OKAPI BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e205399f-4073-41ee-b0aa-f1e72ca70e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_query_processing(query, reverse_index):\n",
    "   \n",
    "    query = query.upper()\n",
    "    query_tokens = re.findall(r'[\\w]+|AND|OR|NOT|\\(|\\)', query)\n",
    "    term_postings = {}\n",
    "    for token in query_tokens:\n",
    "        if token not in {'AND', 'OR', 'NOT', '(', ')'}:\n",
    "            preprocessed_token = preprocess_text(token)[0]\n",
    "            if preprocessed_token in reverse_index:\n",
    "                term_postings[token] = set(reverse_index[preprocessed_token])\n",
    "            else:\n",
    "                term_postings[token] = set()\n",
    "\n",
    "    def eval_query(tokens):\n",
    "        stack = []\n",
    "        for token in tokens:\n",
    "            if token == \"AND\":\n",
    "                b = stack.pop()\n",
    "                a = stack.pop()\n",
    "                stack.append(a & b)\n",
    "            elif token == \"OR\":\n",
    "                b = stack.pop()\n",
    "                a = stack.pop()\n",
    "                stack.append(a | b)\n",
    "            elif token == \"NOT\":\n",
    "                a = stack.pop()\n",
    "                stack.append(set(range(len(reverse_index))) - a)\n",
    "            else:\n",
    "                stack.append(term_postings.get(token, set()))\n",
    "        return stack[0]\n",
    "\n",
    "    def infix_to_postfix(tokens):\n",
    "        precedence = {'NOT': 3, 'AND': 2, 'OR': 1}\n",
    "        output = []\n",
    "        operators = []\n",
    "        for token in tokens:\n",
    "            if token not in precedence and token not in {'(', ')'}:\n",
    "                output.append(token)\n",
    "            elif token == '(':\n",
    "                operators.append(token)\n",
    "            elif token == ')':\n",
    "                while operators and operators[-1] != '(':\n",
    "                    output.append(operators.pop())\n",
    "                operators.pop()\n",
    "            else:\n",
    "                while (operators and operators[-1] != '(' and\n",
    "                       precedence.get(token, 0) <= precedence.get(operators[-1], 0)):\n",
    "                    output.append(operators.pop())\n",
    "                operators.append(token)\n",
    "        while operators:\n",
    "            output.append(operators.pop())\n",
    "        return output\n",
    "\n",
    "    postfix_query = infix_to_postfix(query_tokens)\n",
    "    return list(eval_query(postfix_query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b24a3f-5a8e-488b-b42e-daa267cb84eb",
   "metadata": {},
   "source": [
    "##ΠΑΡΑΔΕΙΓΜΑ ΕΚΤΕΛΕΣΗΣ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5b4a7-8fd3-4487-9b05-83946b9aed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Retrieval\n",
    "query = \"artificial AND intelligence OR learning\"\n",
    "result_docs = boolean_query_processing(query, reverse_index)\n",
    "print(f\"Boolean Retrieval αποτελέσματα: {result_docs}\")\n",
    "\n",
    "# Vector Space Model\n",
    "query = \"artificial intelligence\"\n",
    "vsm_results = vector_space_model(query, reverse_index, articles)\n",
    "print(f\"VSM αποτελέσματα: {vsm_results}\")\n",
    "\n",
    "# Okapi BM25\n",
    "bm25_results = okapi_bm25(query, reverse_index, articles)\n",
    "print(f\"BM25 αποτελέσματα: {bm25_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3a75a-24a1-494a-981f-164c3df4d8a6",
   "metadata": {},
   "source": [
    "#ΣΥΜΠΕΡΑΣΜΑΤΑ\n",
    "Η ΜΕΘΟΔΟΣ BOOLEAN EINAI ΧΡΗΣΙΜΗ ΓΙΑ ΑΚΡΙΒΕΙΣ ΕΡΩΤΗΣΕΙΣ.\n",
    "ΤΟ OKAPI BM 25 ΕΙΝΑΙ ΧΡΗΣΙΜΟ ΓΙΑ ΕΡΩΤΗΜΑΤΑ ΠΟΛΛΑΠΛΩΝ ΛΕΞΕΩΝ\n",
    "ΤΟ VSM XΡΗΣΙΜΟΠΟΙΕΙΤΑΙ ΚΥΡΙΩΣ ΓΙΑ ΜΕΓΑΛΑ ΕΡΩΤΗΜΑΤΑ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
